[core]
# Diretório onde o Airflow armazena suas configurações e arquivos
airflow_home = /usr/local/airflow
# Conexão com o banco de dados
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@postgres/airflow
# Executor a ser usado (CeleryExecutor neste caso)
executor = CeleryExecutor
# Chave Fernet para criptografia de dados sensíveis
fernet_key = zjT6VlDV0ew8-4wJc8oM1hx8Y2PlpDs5mLH7C2Xsy0A=
# Timezone
default_timezone = utc

[webserver]
# Porta onde o webserver do Airflow será executado
web_server_port = 8080
# Interface para exibição dos logs do Webserver
web_server_log_level = info
# Host para o Webserver
web_server_host = 0.0.0.0

[logging]
# Diretório onde os logs são armazenados
base_log_folder = /usr/local/airflow/logs
# Modo dos logs
remote_logging = False

[scheduler]
# Diretório onde os DAGs são armazenados
dags_are_paused_at_creation = False
catchup_by_default = True
max_threads = 2

[celery]
# URL do broker do Celery (Redis neste caso)
broker_url = redis://redis:6379/0
# URL do backend de resultados do Celery
result_backend = db+postgresql://airflow:airflow@postgres/airflow
# Número máximo de tarefas que podem ser executadas em paralelo
worker_concurrency = 16
# Diretório de logs dos workers do Celery
worker_log_server_port = 8793
# Tempo de ping para verificar a disponibilidade dos workers
worker_ping_interval = 30
# Host para o Celery Flower
flower_host = 0.0.0.0
# Porta para o Celery Flower
flower_port = 5555

[worker]
# Número de retries padrão para tarefas
default_celery_task_retries = 3
# Intervalo entre retries das tarefas
celery_retry_delay = 300
