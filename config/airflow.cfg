[core]
# Directory where Airflow stores its configurations and files
airflow_home = /usr/local/airflow
# Connection to the database
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@postgres/airflow
# Executor to be used (CeleryExecutor in this case)
executor = CeleryExecutor
# Fernet key for sensitive data encryption
fernet_key = zjT6VlDV0ew8-4wJc8oM1hx8Y2PlpDs5mLH7C2Xsy0A=
# Timezone
default_timezone = utc

[webserver]
# Port where the Airflow webserver will be running
web_server_port = 8080
# Interface for displaying Webserver logs
web_server_log_level = info
# Host for the Webserver
web_server_host = 0.0.0.0

[logging]
# Directory where logs are stored
base_log_folder = /usr/local/airflow/logs
# Logging mode
remote_logging = False

[scheduler]
# Directory where DAGs are stored
dags_are_paused_at_creation = False
catchup_by_default = True
max_threads = 2

[celery]
# Celery broker URL (Redis in this case)
broker_url = redis://redis:6379/0
# Celery backend URL
result_backend = db+postgresql://airflow:airflow@postgres/airflow
# Maximum number of tasks that can be executed in parallel
worker_concurrency = 16
# Directory for Celery workers logs
worker_log_server_port = 8793
# Ping time to check workers availability
worker_ping_interval = 30
# Host for Celery Flower
flower_host = 0.0.0.0
# Port for Celery Flower
flower_port = 5555

[worker]
# Default number of retries for tasks
default_celery_task_retries = 3
# Interval between task retries
celery_retry_delay = 300
